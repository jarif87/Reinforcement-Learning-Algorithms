{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* State-action-reward-state-action (SARSA) is an on-policy algorithm designed to teach a machine learning model a new Markov decision process policy in order to solve reinforcement learning challenges. It’s an algorithm where, in the current state (S), an action (A) is taken and the agent gets a reward (R), and ends up in the next state (S1), and takes action (A1) in S1. Therefore, the tuple (S, A, R, S1, A1) stands for the acronym SARSA. \n\n* It’s called an on-policy algorithm because it updates the policy based on actions taken.","metadata":{}},{"cell_type":"markdown","source":"# What Is SARSA?\n\n> SARSA is an on-policy algorithm used in reinforcement learning to train a Markov decision process model on a new policy. It’s an algorithm where, in the current state (S), an action (A) is taken and the agent gets a reward (R), and ends up in the next state (S1), and takes action (A1) in S1, or in other words, the tuple S, A, R, S1, A1.","metadata":{}},{"cell_type":"markdown","source":"# How Does the SARSA Algorithm Work? \n\n* The SARSA algorithm works by carrying out actions based on rewards received from previous actions. To do this, SARSA stores a table of state (S)-action (A) estimate pairs for each Q-value. This table is known as a Q-table, while the state-action pairs are denoted as Q(S, A). \n\n* The SARSA process starts by initializing Q(S, A) to arbitrary values. In this step, the initial current state (S) is set, and the initial action (A) is selected by using an epsilon-greedy algorithm policy based on current Q-values. An epsilon-greedy policy balances the use of exploitation and exploration methods in the learning process to select the action with the highest estimated reward. \n\n* Exploitation involves using already known, estimated values to get more previously earned rewards in the learning process. Exploration involves attempting to find new knowledge on actions, which may result in short-term, sub-optimal actions during learning but may yield long-term benefits to find the best possible action and reward.\n\n* From here, the selected action is taken, and the reward (R) and next state (S1) are observed. Q(S, A) is then updated, and the next action (A1) is selected based on the updated Q-values. Action-value estimates of a state are also updated for each current action-state pair present, which estimates the value of receiving a reward for taking a given action.\n\n* The above steps of R through A1 are repeated until the algorithm’s given episode ends, which describes the sequence of states, actions and rewards taken until the final (terminal) state is reached. State, action and reward experiences in the SARSA process are used to update Q(S, A) values for each iteration.","metadata":{}},{"cell_type":"code","source":"import gym\nimport numpy as np\nimport time\nimport os\nfrom tqdm import tqdm  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:47:20.554954Z","iopub.execute_input":"2025-03-05T12:47:20.555305Z","iopub.status.idle":"2025-03-05T12:47:20.560429Z","shell.execute_reply.started":"2025-03-05T12:47:20.555279Z","shell.execute_reply":"2025-03-05T12:47:20.558981Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"\n# Define the custom FrozenLake map","metadata":{}},{"cell_type":"code","source":"custom_map = [\n    \"SFFF\",\n    \"FHFH\",\n    \"FFFH\",\n    \"HFFG\"\n]\n\n# Create the environment\nenv = gym.make('FrozenLake-v1', desc=custom_map, is_slippery=False)\n\n# Q-learning parameters (optimized)\nepsilon = 0.9\ntotal_episodes = 75000  # Increased for robust learning\nmax_steps = 100\nlr_rate = 0.5  # Reduced for stable updates\ngamma = 0.99  # Higher value to prioritize long-term reward\nepsilon_min = 0.01\nepsilon_decay = 0.9995  # Very slow decay for extensive exploration","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:47:20.564456Z","iopub.execute_input":"2025-03-05T12:47:20.564809Z","iopub.status.idle":"2025-03-05T12:47:20.581484Z","shell.execute_reply.started":"2025-03-05T12:47:20.564779Z","shell.execute_reply":"2025-03-05T12:47:20.580310Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"# Initialize Q-table","metadata":{}},{"cell_type":"code","source":"\nQ = np.zeros((env.observation_space.n, env.action_space.n))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:47:20.583022Z","iopub.execute_input":"2025-03-05T12:47:20.583417Z","iopub.status.idle":"2025-03-05T12:47:20.601651Z","shell.execute_reply.started":"2025-03-05T12:47:20.583365Z","shell.execute_reply":"2025-03-05T12:47:20.600508Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"\n# Function to choose an action with epsilon decay","metadata":{}},{"cell_type":"code","source":"\ndef choose_action(state):\n    if np.random.uniform(0, 1) < epsilon:\n        return env.action_space.sample()  # Random exploration\n    return np.argmax(Q[state, :])  # Greedy action\n\n# Function to update Q-table\ndef learn(state, state2, reward, action, action2):\n    predict = Q[state, action]\n    target = reward + gamma * Q[state2, action2]\n    Q[state, action] += lr_rate * (target - predict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:47:20.604354Z","iopub.execute_input":"2025-03-05T12:47:20.604736Z","iopub.status.idle":"2025-03-05T12:47:20.625540Z","shell.execute_reply.started":"2025-03-05T12:47:20.604706Z","shell.execute_reply":"2025-03-05T12:47:20.624250Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"\n# Training loop with tqdm progress bar","metadata":{}},{"cell_type":"code","source":"\ntotal_rewards = 0\nsuccessful_episodes = 0\nwith tqdm(total=total_episodes, desc=\"Training Progress\") as pbar:\n    for episode in range(total_episodes):\n        state = env.reset()\n        action = choose_action(state)\n        episode_reward = 0\n\n        for t in range(max_steps):\n            state2, reward, done, _ = env.step(action)\n            action2 = choose_action(state2)\n            learn(state, state2, reward, action, action2)\n\n            state = state2\n            action = action2\n            episode_reward += reward\n\n            if done:\n                if reward == 1:\n                    successful_episodes += 1\n                break\n\n        total_rewards += episode_reward\n        epsilon = max(epsilon_min, epsilon * epsilon_decay)  # Decay epsilon\n\n        # Update progress bar\n        pbar.update(1)\n        if episode % 10000 == 0:\n            pbar.set_postfix({\"Total Rewards\": total_rewards, \"Success Rate\": f\"{successful_episodes / (episode + 1):.2%}\"})\n\nprint(\"\\nTraining complete! Total rewards:\", total_rewards, \"Success Rate:\", successful_episodes / total_episodes)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:47:20.626859Z","iopub.execute_input":"2025-03-05T12:47:20.627311Z","iopub.status.idle":"2025-03-05T12:47:38.818441Z","shell.execute_reply.started":"2025-03-05T12:47:20.627280Z","shell.execute_reply":"2025-03-05T12:47:38.816647Z"}},"outputs":[{"name":"stderr","text":"Training Progress: 100%|██████████| 75000/75000 [00:18<00:00, 4128.28it/s, Total Rewards=65844.0, Success Rate=94.06%]","output_type":"stream"},{"name":"stdout","text":"\nTraining complete! Total rewards: 70749.0 Success Rate: 0.94332\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"# ---- Enhanced Visualization Code with Red Indicator ----","metadata":{}},{"cell_type":"code","source":"\ndef render_grid(state, prev_state=None):\n    \"\"\"Prints the FrozenLake grid with the agent's position and red previous position\"\"\"\n    os.system('clear' if os.name == 'posix' else 'cls')  # Clears the screen\n    grid_size = 4\n    grid = [list(row) for row in custom_map]\n\n    # Mark current agent position\n    row, col = divmod(state, grid_size)\n    grid[row][col] = \"A\"  # Current position as \"A\"\n\n    # Mark previous position in red if provided\n    if prev_state is not None and prev_state != state:\n        prev_row, prev_col = divmod(prev_state, grid_size)\n        if 0 <= prev_row < grid_size and 0 <= prev_col < grid_size:  # Ensure within bounds\n            grid[prev_row][prev_col] = \"\\033[31m\" + grid[prev_row][prev_col] + \"\\033[0m\"  # Red color\n\n    # Print the grid\n    for line in grid:\n        print(\" \".join(line))\n    print()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:47:38.819418Z","iopub.execute_input":"2025-03-05T12:47:38.819809Z","iopub.status.idle":"2025-03-05T12:47:38.828106Z","shell.execute_reply.started":"2025-03-05T12:47:38.819778Z","shell.execute_reply":"2025-03-05T12:47:38.826108Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"# Run the agent dynamically with a full path and red indicator","metadata":{}},{"cell_type":"code","source":"\ndef visualize_agent(Q, env):\n    state = env.reset()\n    done = False\n    step_count = 0\n    max_steps = 20  # Limit steps to avoid infinite loops\n    prev_state = None  # Initialize previous state as None to start\n\n    print(\"Starting visualization of agent's learned path...\\n\")\n    while not done and step_count < max_steps:\n        render_grid(state, prev_state)  # Render with previous state in red\n        \n        # Determine movement direction dynamically\n        prev_row, prev_col = divmod(prev_state, 4) if prev_state is not None else (-1, -1)\n        row, col = divmod(state, 4)\n\n        if row > prev_row:\n            move = \"DOWN\"\n        elif row < prev_row:\n            move = \"UP\"\n        elif col > prev_col:\n            move = \"RIGHT\"\n        else:\n            move = \"LEFT\"\n\n        print(f\"Agent moves: {move}\\n\")\n        \n        prev_state = state  # Update previous state before moving\n        action = np.argmax(Q[state])  # Use the learned policy\n        state, reward, done, _ = env.step(action)\n        step_count += 1\n        time.sleep(0.5)  # Pause for effect\n\n    render_grid(state)  # Show final state (no previous state needed)\n    if reward == 1:\n        print(\"🏆 Goal reached! Frisbee retrieved!\")\n    elif done:\n        print(\"💀 Fell in a hole!\")\n    else:\n        print(\"❌ Max steps reached without goal.\")\n\n# Run visualization\nvisualize_agent(Q, env)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:47:38.829498Z","iopub.execute_input":"2025-03-05T12:47:38.830143Z","iopub.status.idle":"2025-03-05T12:47:41.902484Z","shell.execute_reply.started":"2025-03-05T12:47:38.830088Z","shell.execute_reply":"2025-03-05T12:47:41.901331Z"}},"outputs":[{"name":"stdout","text":"Starting visualization of agent's learned path...\n\nA F F F\nF H F H\nF F F H\nH F F G\n\nAgent moves: DOWN\n\n\u001b[31mS\u001b[0m F F F\nA H F H\nF F F H\nH F F G\n\nAgent moves: DOWN\n\nS F F F\n\u001b[31mF\u001b[0m H F H\nA F F H\nH F F G\n\nAgent moves: DOWN\n\nS F F F\nF H F H\n\u001b[31mF\u001b[0m A F H\nH F F G\n\nAgent moves: RIGHT\n\nS F F F\nF H F H\nF \u001b[31mF\u001b[0m F H\nH A F G\n\nAgent moves: DOWN\n\nS F F F\nF H F H\nF F F H\nH \u001b[31mF\u001b[0m A G\n\nAgent moves: RIGHT\n\nS F F F\nF H F H\nF F F H\nH F F A\n\n🏆 Goal reached! Frisbee retrieved!\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}